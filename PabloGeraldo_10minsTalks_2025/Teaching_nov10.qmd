---
title: "`An invitation to Supervised Machine Learning with Trees`"
author: "Pablo Geraldo Bast√≠as \n

(pablo.geraldo@nuffield.ox.ac.uk)"
#logo: "https://fundit.fr/sites/default/files/styles/max_650x650/public/institutions/capture-decran-2023-07-07-162216.png?itok=1CkwlJEu"
# include-in-header:
#   - text: |
#       <style>
#       .reveal .slide-logo {
#         max-height: unset;
#         height: 80px;
#       }
#       </style>
#footer: "Teaching presentation - UCL QSS"
date: 11/10/2025
date-format: long
title-slide-attributes:
  data-background-image: ./img/pexels-felix-mittermeier-957024.jpg
  data-background-opacity: "0.5"
  data-background-size: cover
  #data-background-color: "#95C11F"
format: 
  revealjs:
    theme: simple
    width: 1600
    height: 900
    #transition: slide
    slide-number: c/t
#    chalkboard: true
#    auto-stretch: false
callout-appearance: minimal
---


## Learning objectives

* Understand the basic *ingredients* of Supervised Machine Learning: 

  - A collection of labelled examples
  - [A learning algorithm]{.fragment .highlight-blue} 
  - [An evaluation metric]{.fragment .highlight-red}
  - [A testing procedure]{.fragment .highlight-green}
  
. . .
  
* Understand how classification and regression trees work:

  - [Recursive partitioning]{.fragment .highlight-blue}
  - [Information gain]{.fragment .highlight-red} 
  - [Cross-validation loss]{.fragment .highlight-green}
  
. . .

* A brief preview of regularization and ensemble methods


# Supervised Learning {background-color="#00a191"}

## Types of Machine Learning

<br>

Machine learning is usually divided into three categories

::: {.columns}

::: {.column width="33%"}

[*Supervised* learning]{.fragment .highlight-blue}

::: {.r-stack}

![](img/doodle_SL1.png){.fragment fig-align="left" width="40%"}

![](img/doodle_SL2.png){.fragment fig-align="left" width="40%"}
:::

[`Input` $\rightarrow$ `Output`]{.fragment}

:::

::: {.column width="33%"}
[*Unsupervised* learning]{.fragment .highlight-blue}

::: {.r-stack}

![](img/doodle_UL1.png){.fragment fig-align="left" width="40%"}

![](img/doodle_UL2.png){.fragment fig-align="left" width="40%"}

:::

[`Input` $\rightarrow$ `Structure`]{.fragment}

:::

::: {.column width="33%"}
[*Reinforcement* learning]{.fragment .highlight-blue}


::: {.r-stack}

![](img/doodle_RL1.png){.fragment fig-align="left" width="40%"}

![](img/doodle_RL2.png){.fragment fig-align="left" width="45%"}

:::

[`Action` $\rightarrow$ `Reward` $\rightarrow$ `Action'`]{.fragment}

:::

:::


# `Lesson 1: the basic ingredient of SL is labelled data`
  
## How does a Machine Learn?

<br>


[**Training**: showing the machine a collection of examples that *includes* the correct solution]{.fragment .highlight-blue}
  
  + **Requirement**: a metric of how bad our guesses are ("loss")
  
  
::: {.columns}

::: {.column width="50%" .fragment}

A common error metric is the Mean Squared Error:

$$
MSE = \frac{1}{n} \sum_i^n (y_i - \hat{y_i})^2
$$

:::

::: {.column width="50%" .fragment}

  
```{r}
#| echo: false
#| code-fold: true

library(knitr)

set.seed(1234)
n <- 10
b <- 2.5
X1 <- rnorm(n)
X2 <- rnorm(n, mean = 5, sd = 2)
prob <- exp(b*(X1+X2)) / (1 + exp(b*(X1+X2)))
#y_class <- 1*(runif(n)<prob)
y_num <- 0.5 + X1*0.2 - X2*0.8 + rnorm(n)

#class <- glm(y ~ X, family = binomial)
#summary(class)

df <- data.frame(X1 = X1, X2 = X2,
                 #y_class = y_class,
                 y_num = y_num)
m1 <- lm(y_num ~ X1 + X2, data = df)
df$y_hat <- predict(m1, data = df)
df$sq_err <- (df$y_num - df$y_hat)^2

kable(round(head(df), 2))
```

:::


:::


# `Lesson 2: the focus of SL is predicting the correct output`

## What is Supervised Learning for?

<br>

[**Goal**: building a prediction machine `(Input` $\rightarrow$ `Output)`]{.fragment .highlight-blue}

  + **Requirement**: select an *algorithm* from the class of available machines
  + In other words, define the "hypothesis space"
  
<br>

. . .
  
::: {.columns}

::: {.column width="50%"}

For example, we could assume that the relationship between our inputs ($X$) and the output ($y$) is linear, and fit something like:

$$
y_i = \alpha + \beta_1 X_1 + \beta_2 X_2 + \epsilon_i
$$
:::


::: {.column width="50%" .fragment}

```{r}
#| echo: false
#| code-fold: true

library(dplyr)
library(ggplot2)

# Data generating process
true_conditional_mean <- tibble(z = F, x = seq(0,1,.001)) |>
  bind_rows(tibble(z = T, x = seq(0,1,.001))) |>
  mutate(mu = z * plogis(10 * (x - .5)))

simulate <- function(sample_size) {
  tibble(z = F, x = seq(0,1,.001)) |>
    bind_rows(tibble(z = T, x = seq(0,1,.001))) |>
    mutate(mu = z * plogis(10 * (x - .5))) |>
    slice_sample(n = sample_size, replace = T) |>
    mutate(y = mu + rnorm(n(), sd = .1))
}
simulated_data <- simulate(1000)
p_no_points <- true_conditional_mean |>
  ggplot(aes(x = x, color = z, y = mu)) +
  geom_line(linetype = "dashed", size = 1.2) +
  labs(
    x = "Numeric Predictor X1",
    y = "Numeric Outcome Y",
    color = "Binary Predictor 2"
  ) +
  theme_bw()
p <- p_no_points +
  geom_point(data = simulated_data, aes(y = y), size = .2, alpha = .3)

best_linear_fit <- lm(mu ~ x + z, data = true_conditional_mean)
p +
  geom_line(
    data = true_conditional_mean |>
      mutate(mu = predict(best_linear_fit))
  ) +
  theme_bw() +
  ggtitle("An additive linear model (solid lines) poorly approximates\nthe true conditional mean function (dashed lines)")
```
:::

:::

# `Lesson 3: the key to good SL is testing our predictions`
  
## How do we check our Machine?

<br>


[**Testing**: assessing how good our predictions are in *unseen* examples]{.fragment .highlight-blue}

  + **Requirement**: `newData` so we can test `newInput` $\rightarrow$ `newOutput` 
  + There are some tricks for this!
  

::: {.columns}

::: {.column width="50%" .fragment}

![](img/train_test.png){.fragment fig-align="left" width="45%"}

:::

::: {.column width="50%" .fragment}

![](img/kfold_cv.png){.fragment fig-align="left" width="40%"}

:::

:::




# Tree-based methods {background-color="#00a191"}

## Recursive partitioning


<br>

[In decision trees, our purpose is to split the data in partitions that are *disimilar* between each other, but *similar* within]{.fragment .highlight-blue}
  
  
::: {.columns}

::: {.column width="50%" .fragment}

* The Gini Coefficient

$$
Gini = 1 - \sum (p_i)^2
$$

* Entropy

$$
H(t) = -\sum \{P[t = i] \times \log_2 (P[t=i])\}
$$

:::

::: {.column width="50%" .fragment}

![](img/entropy_cards.png){.fragment fig-align="left" width="70%"}

:::


:::


## Evaluate these partitions

![](img/guess_who_long.png)

## Evaluate these partitions

![](img/guess_who_short.png)


## Tree depth

![](img/tree_d2.png)

## Tree depth

![](img/tree_d4.png)

## Decision rules

![](img/tree_rep.png)


## Strenghts and weaknesses


::: {.columns}

::: {.column width="50%" .fragment}

[The good]{.fragment .highlight-blue}

* Very easy to explain

* Interpretable decision rules

* Handles well categorical variables

* Handles well non linearities

* Cheap to run

:::


::: {.column width="50%" .fragment}

[The bad]{.fragment .highlight-blue}

* High variance predictions

* Greedy algorithm

* Doesn't handle well linear relationships

* Limited accuracy

:::


:::


## Extensions: Regularization and Ensembles

::: {.columns}

::: {.column width="50%" .fragment}

[Regularization]{.fragment .highlight-blue}

To control for high variance, we can do different things:

* Stop the tree from growing!

  + Maximum number of steps
  
  + Minimum number of obs in a leaf
  
  + Maximum number of nodes
  
  + Minimum decrease in loss
  

* Or let of grow and pune it back! 

:::

::: {.column width="50%" .fragment}

[Ensembles]{.fragment .highlight-blue}

Better than a tree... in many trees!

* *Bagging*: Random Forest

  - "Bootstrap aggregation"
  
  - Train in subsamples, then aggregate
  
  - Can also sample covariates
  
  - More bias, less variance
  
* *Boosting*: XGboost, Adaboost

  - Feed predictions back to tree
  
  - Partition to predict difficult cases

:::

:::

# Summary {background-color="#00a191"}

## Review


* Understand the basic *ingredients* of Supervised Machine Learning: 

  - A collection of labelled examples
  - [A learning algorithm]{.fragment .highlight-blue} 
  - [An evaluation metric]{.fragment .highlight-red}
  - [A testing procedure]{.fragment .highlight-green}
  
. . .
  
* Understand how classification and regression trees work:

  - [Recursive partitioning]{.fragment .highlight-blue}
  - [Information gain]{.fragment .highlight-red} 
  - [Cross-validation loss]{.fragment .highlight-green}
  
. . .

* A brief preview of regularization and ensemble methods




