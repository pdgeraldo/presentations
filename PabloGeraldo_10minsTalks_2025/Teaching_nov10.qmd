---
title: "`An invitation to Supervised Machine Learning with Trees`"
author: "Pablo Geraldo Bast√≠as \n

(pablo.geraldo@nuffield.ox.ac.uk)"
#logo: "https://fundit.fr/sites/default/files/styles/max_650x650/public/institutions/capture-decran-2023-07-07-162216.png?itok=1CkwlJEu"
# include-in-header:
#   - text: |
#       <style>
#       .reveal .slide-logo {
#         max-height: unset;
#         height: 80px;
#       }
#       </style>
footer: "Teaching presentation - UCL QSS"
date: 11/10/2025
date-format: long
title-slide-attributes:
  data-background-image: ./img/pexels-felix-mittermeier-957024.jpg
  data-background-opacity: "0.5"
  data-background-size: cover
  #data-background-color: "#95C11F"
format: 
  revealjs:
    theme: simple
    width: 1600
    height: 900
    #transition: slide
    slide-number: c/t
#    chalkboard: true
#    auto-stretch: false
callout-appearance: minimal
---


## Learning objectives


# Supervised Learning {background-color="#00a191"}

## The ingredients of Supervised Learning

<br>

[**Goal**: building a prediction machine `Input` $\rightarrow$ `Output`]{.fragment .highlight-blue}

  + **Requirement**: select an *algorithm* from the class of available machines
  + In other words, define the "hypothesis space"
  
<br>

. . .
  
::: {.columns}

::: {.column width="50%"}

For example, we could assume that the relationship between our inputs ($X$) and the output ($y$) is linear, and fit something like:

$$
y_i = \alpha + \beta_1 X_1 + \beta_2 X_2 + \epsilon_i
$$
:::


::: {.column width="50%" .fragment}

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "expand for code"

library(dplyr)
library(ggplot2)

# Data generating process
true_conditional_mean <- tibble(z = F, x = seq(0,1,.001)) |>
  bind_rows(tibble(z = T, x = seq(0,1,.001))) |>
  mutate(mu = z * plogis(10 * (x - .5)))

simulate <- function(sample_size) {
  tibble(z = F, x = seq(0,1,.001)) |>
    bind_rows(tibble(z = T, x = seq(0,1,.001))) |>
    mutate(mu = z * plogis(10 * (x - .5))) |>
    slice_sample(n = sample_size, replace = T) |>
    mutate(y = mu + rnorm(n(), sd = .1))
}
simulated_data <- simulate(1000)
p_no_points <- true_conditional_mean |>
  ggplot(aes(x = x, color = z, y = mu)) +
  geom_line(linetype = "dashed", size = 1.2) +
  labs(
    x = "Numeric Predictor X",
    y = "Numeric Outcome Y",
    color = "Binary Predictor Z"
  ) +
  theme_bw()
p <- p_no_points +
  geom_point(data = simulated_data, aes(y = y), size = .2, alpha = .3)
p
```
:::

:::

  
  



  
## The ingredients of Supervised Learning

<br>


[**Method**: training the machine to learn the right "$\rightarrow$" from examples]{.fragment .highlight-blue}
  
  + **Requirement**: a collection of examples that *includes* the solution
  + $(\textbf{x}_i,y_i)$ for $i=1, \dots, n$
  
  
## The ingredients of Supervised Learning

<br>


[**Evaluation**: assessing how good our predictions are. Can we do better?]{.fragment .highlight-blue}

  + **Requirement**: `newData` so we can test `newInput` $\rightarrow$ `newOutput` 
  + There are some tricks for this!

# Tree-based methods {background-color="#00a191"}

# Extensions {background-color="#00a191"}

# Summary {background-color="#00a191"}



