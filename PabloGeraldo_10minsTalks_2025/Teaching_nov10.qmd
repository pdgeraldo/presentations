---
title: "`An invitation to Supervised Machine Learning with Trees`"
author: "Pablo Geraldo Bast√≠as \n

(pablo.geraldo@nuffield.ox.ac.uk)"
#logo: "https://fundit.fr/sites/default/files/styles/max_650x650/public/institutions/capture-decran-2023-07-07-162216.png?itok=1CkwlJEu"
# include-in-header:
#   - text: |
#       <style>
#       .reveal .slide-logo {
#         max-height: unset;
#         height: 80px;
#       }
#       </style>
#footer: "Teaching presentation - UCL QSS"
date: 11/10/2025
date-format: long
title-slide-attributes:
  data-background-image: ./img/pexels-felix-mittermeier-957024.jpg
  data-background-opacity: "0.5"
  data-background-size: cover
  #data-background-color: "#95C11F"
format: 
  revealjs:
    theme: simple
    width: 1600
    height: 900
    #transition: slide
    slide-number: c/t
#    chalkboard: true
#    auto-stretch: false
callout-appearance: minimal
---


## Learning objectives

* Understand the basic *ingredients* of Supervised Machine Learning: 

  - A collection of labelled examples
  - [A learning algorithm]{.fragment .highlight-blue} 
  - [An evaluation metric]{.fragment .highlight-red}
  - [A testing procedure]{.fragment .highlight-green}
  
. . .
  
* Understand how classification and regression trees work:

  - [Recursive partitioning]{.fragment .highlight-blue}
  - [Information gain]{.fragment .highlight-red} 
  - [Cross-validation loss]{.fragment .highlight-green}
  
. . .

* A brief preview of regularization and ensemble methods


# Supervised Learning {background-color="#00a191"}

## Types of Machine Learning

<br>

Machine learning is usually divided into three categories

::: {.columns}

::: {.column width="33%"}

[*Supervised* learning]{.fragment .highlight-blue}

::: {.r-stack}

![](img/doodle_SL1.png){.fragment fig-align="left" width="40%"}

![](img/doodle_SL2.png){.fragment fig-align="left" width="40%"}
:::

[`Input` $\rightarrow$ `Output`]{.fragment}

:::

::: {.column width="33%"}
[*Unsupervised* learning]{.fragment .highlight-blue}

::: {.r-stack}

![](img/doodle_UL1.png){.fragment fig-align="left" width="40%"}

![](img/doodle_UL2.png){.fragment fig-align="left" width="40%"}

:::

[`Input` $\rightarrow$ `Structure`]{.fragment}

:::

::: {.column width="33%"}
[*Reinforcement* learning]{.fragment .highlight-blue}


::: {.r-stack}

![](img/doodle_RL1.png){.fragment fig-align="left" width="40%"}

![](img/doodle_RL2.png){.fragment fig-align="left" width="45%"}

:::

[`Action` $\rightarrow$ `Reward` $\rightarrow$ `Action'`]{.fragment}

:::

:::

  
## How does a Machine Learn?

<br>


[**Training**: showing the machine a collection of examples that *includes* the correct solution]{.fragment .highlight-blue}
  
  + **Requirement**: a metric of how bad our guesses are ("loss")
  
  
::: {.columns}

::: {.column width="50%" .fragment}

  
```{r}
#| echo: false
#| code-fold: true

library(knitr)

df <- data.frame(X1 = rnorm(10),
                 X2 = rnorm(10, mean = 5, sd = 2),
                 y = rbinom(10, 2, 0.5))

kable(head(df))
```

:::

::: {.column width="50%" .fragment}

A common error metric is the Mean Squared Error:

$$
MSE = \frac{1}{n} \sum_i^n (y_i - \hat{y_i})^2
$$

:::

:::


# `Lesson 1: the basic ingredient of SL is labelled data`


## What is Supervised Learning for?

<br>

[**Goal**: building a prediction machine `(Input` $\rightarrow$ `Output)`]{.fragment .highlight-blue}

  + **Requirement**: select an *algorithm* from the class of available machines
  + In other words, define the "hypothesis space"
  
<br>

. . .
  
::: {.columns}

::: {.column width="50%"}

For example, we could assume that the relationship between our inputs ($X$) and the output ($y$) is linear, and fit something like:

$$
y_i = \alpha + \beta_1 X_1 + \beta_2 X_2 + \epsilon_i
$$
:::


::: {.column width="50%" .fragment}

```{r}
#| echo: false
#| code-fold: true

library(dplyr)
library(ggplot2)

# Data generating process
true_conditional_mean <- tibble(z = F, x = seq(0,1,.001)) |>
  bind_rows(tibble(z = T, x = seq(0,1,.001))) |>
  mutate(mu = z * plogis(10 * (x - .5)))

simulate <- function(sample_size) {
  tibble(z = F, x = seq(0,1,.001)) |>
    bind_rows(tibble(z = T, x = seq(0,1,.001))) |>
    mutate(mu = z * plogis(10 * (x - .5))) |>
    slice_sample(n = sample_size, replace = T) |>
    mutate(y = mu + rnorm(n(), sd = .1))
}
simulated_data <- simulate(1000)
p_no_points <- true_conditional_mean |>
  ggplot(aes(x = x, color = z, y = mu)) +
  geom_line(linetype = "dashed", size = 1.2) +
  labs(
    x = "Numeric Predictor X",
    y = "Numeric Outcome Y",
    color = "Binary Predictor Z"
  ) +
  theme_bw()
p <- p_no_points +
  geom_point(data = simulated_data, aes(y = y), size = .2, alpha = .3)
p
```
:::

:::

  
# `Lesson 2: the focus of SL is predicting the correct output`

  
## How do we check our Machine?

<br>


[**Testing**: assessing how good our predictions are in *unseen* examples]{.fragment .highlight-blue}

  + **Requirement**: `newData` so we can test `newInput` $\rightarrow$ `newOutput` 
  + There are some tricks for this!
  

::: {.columns}

::: {.column width="50%" .fragment}

Explain train / test split

:::

::: {.column width="50%" .fragment}

Explain K-fold CV

:::

:::



# `Lesson 3: the key to good SL is testing our predictions`



# Tree-based methods {background-color="#00a191"}

## Recursive partitioning

## Computing information gain

## Growing the tree

## Evaluating the tree

## Strenghts and weaknesses

* Risk of overfitting 

* Better for non linear relationships

# Extensions {background-color="#00a191"}

## Tree regularization

## Ensembles of trees

## Uses beyond prediction

* Explainability

* Heterogeneous treatment effects

# Summary {background-color="#00a191"}




## Three(*) news about Supervised Learning

<br>

The good news: you have already been doing it!

::: {.columns}

::: {.column width="50%"}
[*Regression*: predict numerical outcome]{.fragment .highlight-blue}

`reg <- lm(y ~ X, data = Data)`
`summary(reg)`

```{r, echo=FALSE}
set.seed(1234)
n <- 10
X <- rnorm(n)
y <- 2.5 * X + rnorm(n)
reg <- lm(y ~ X)
summary(reg)
```
:::

::: {.column width="50%"}
[*Classification*: predict categories]{.fragment .highlight-blue}

`class <- glm(y ~ X, data = Data, family = binomial)`
`summary(class)`


```{r}
set.seed(1234)
n <- 10
b <- 2.5
X <- rnorm(n)
prob <- exp(b*X) / (1 + exp(b*X))
y <- 1*(runif(n)<prob)
class <- glm(y ~ X, family = binomial)
summary(class)
```
:::

:::


## Three(*) news about Supervised Learning

<br>

The bad news: it gets more complicated!

  * Switch focus from $\beta$s to $\hat{Y}$
  * What is wrong here?
  


## Three(*) news about Supervised Learning

* [The better news: the building blocks remain simple!]{.fragment .highlight-blue}


::: aside
(*) No pun intended
:::


